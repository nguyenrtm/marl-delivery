{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!git clone https://github.com/cuongtv312/marl-delivery.git\n%cd marl-delivery\n!pip install -r requirements.txt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:41.991950Z","iopub.execute_input":"2025-05-15T16:39:41.992213Z","iopub.status.idle":"2025-05-15T16:39:48.454521Z","shell.execute_reply.started":"2025-05-15T16:39:41.992195Z","shell.execute_reply":"2025-05-15T16:39:48.453502Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from env import Environment\nimport torch\nfrom collections import deque\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nimport numpy as np\nimport random\nimport os\nfrom sklearn.calibration import LabelEncoder\nimport copy\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:48.455557Z","iopub.execute_input":"2025-05-15T16:39:48.455869Z","iopub.status.idle":"2025-05-15T16:39:56.690475Z","shell.execute_reply.started":"2025-05-15T16:39:48.455832Z","shell.execute_reply":"2025-05-15T16:39:56.689928Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparams","metadata":{}},{"cell_type":"code","source":"SEED = 2025\n\nACTION_DIM = 15\nNUM_AGENTS = 10\nMAP_FILE = \"map5.txt\"\nN_PACKAGES = 1000\n\nMOVE_COST = -0.01\nDELIVERY_REWARD = 10\nDELAY_REWARD = 1\nMAX_TIME_STEPS = 1000\n\nNUM_EPISODES = 1000\nBATCH_SIZE = 64\nGAMMA = 0.99\nLR = 5e-5\nWEIGHT_DECAY = 1e-4\nMAX_REPLAY_BUFFER_SIZE = 10000\nEPS_START = 1\nEPS_END = 0.1\nEPS_DECAY = 1000\nTAU = 0.01\nGRADIENT_CLIPPING = 10\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nnp.random.seed(SEED)\nrandom.seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.692269Z","iopub.execute_input":"2025-05-15T16:39:56.692771Z","iopub.status.idle":"2025-05-15T16:39:56.790088Z","shell.execute_reply.started":"2025-05-15T16:39:56.692751Z","shell.execute_reply":"2025-05-15T16:39:56.789539Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert State\n","metadata":{}},{"cell_type":"code","source":"def manhattan_distance(x1, x2, y1, y2):\n    return abs(x2 - x1) + abs(y2 - y1)\n\ndef bfs_distance(map_grid, start_x, start_y, goal_x, goal_y): #0-indexed\n    start = (start_x, start_y)\n    goal = (goal_x, goal_y)\n    \n    n_rows = len(map_grid)\n    n_cols = len(map_grid[0])\n    queue = deque([(goal, 0)])\n    visited = set([goal])\n\n    while queue:\n        current, dist = queue.popleft()\n        if current == start:\n            return dist\n\n        for dx, dy in [(-1, 0), (1, 0), (0, -1), (0, 1)]:\n            next_pos = (current[0] + dx, current[1] + dy)\n            if 0 <= next_pos[0] < n_rows and 0 <= next_pos[1] < n_cols:\n                if map_grid[next_pos[0]][next_pos[1]] == 0 and next_pos not in visited:\n                    visited.add(next_pos)\n                    queue.append((next_pos, dist + 1))\n\n    return float('inf')  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:42:24.421999Z","iopub.execute_input":"2025-05-15T16:42:24.422300Z","iopub.status.idle":"2025-05-15T16:42:24.429140Z","shell.execute_reply.started":"2025-05-15T16:42:24.422279Z","shell.execute_reply":"2025-05-15T16:42:24.427941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def convert_state(state, persistent_packages, current_robot_idx):\n    \"\"\"\n    Convert robot's observation to a 2D multi-channel tensor.\n    - 6 channels for robot-specific observation:\n        0. Map\n        1. Urgency of 'waiting' packages (if robot is not carrying)\n        2. Start positions of 'waiting' packages (if robot is not carrying)\n        3. Other robots' positions\n        4. Current robot's position\n        5. Current robot's carried package target (if robot is carrying)\n\n    Args:\n        state (dict): Raw state from the environment. Expected keys: \"map\", \"robots\", \"time_step\".\n                      state[\"robots\"] is a list of tuples: (x+1, y+1, carrying_package_id)\n        prev_state(dict): previous state\n        persistent_packages (dict): Dict tracking all active packages.\n        current_robot_idx (int): Index of the current robot.\n\n    Returns:\n        np.ndarray of shape (6, n_rows, n_cols)\n    \"\"\"\n    grid = np.array(state[\"map\"])\n    n_rows, n_cols = grid.shape\n    n_channels = 6\n    tensor = np.zeros((n_channels, n_rows, n_cols), dtype=np.float32)\n\n    #Channel 0: Map\n    tensor[0] = grid\n\n    current_time_step = state[\"time_step\"]\n    if isinstance(current_time_step, np.ndarray): \n        current_time_step = current_time_step[0]\n\n    if current_robot_idx < 0 or current_robot_idx >= len(state[\"robots\"]):\n        print(\"Invalid robot idx\")\n        return tensor \n\n    #current robot data\n    current_robot_data = state[\"robots\"][current_robot_idx]\n    carried_pkg_id_by_current_robot = current_robot_data[2] \n\n    #Channel 1: Urgency of 'waiting' packages (if not carrying) \n    #Channel 2: Start positions of 'waiting' packages (if not carrying)\n    if carried_pkg_id_by_current_robot == 0: # Robot is not carrying\n        for pkg_id, pkg_data in persistent_packages.items():\n            if pkg_data['status'] == 'waiting':\n                # all 1-indexed\n                rr, rc, _ = current_robot_data\n                dr, dc = pkg_data['target_pos']\n                sr, sc = pkg_data['start_pos'] \n                st = pkg_data['start_time']\n                dl = pkg_data['deadline']\n\n                travel_length = bfs_distance(grid, rr - 1, rc - 1, sr - 1, sc - 1) + bfs_distance(grid, sr - 1, sc - 1, dr - 1, dc - 1)\n                time_left = dl - current_time_step - travel_length\n                #check active package\n                if current_time_step >= st:\n                    urgency = 0\n                    # If no time left then no urgency\n                    if dl > st and time_left > 0:\n                        urgency = 1.0/time_left\n      \n\n                    if 0 <= sr < n_rows and 0 <= sc < n_cols: \n                        tensor[1, sr, sc] = max(tensor[1, sr, sc], urgency) \n\n                    # Channel 2: Start position\n                    if 0 <= sr < n_rows and 0 <= sc < n_cols: \n                        tensor[2, sr, sc] = 1.0 \n    \n\n    # Channel 3: Other robots' positions\n    for i, rob_data in enumerate(state[\"robots\"]):\n        if i == current_robot_idx:\n            continue \n        rr, rc, _ = rob_data \n        rr_idx, rc_idx = int(rr) - 1, int(rc) - 1 \n        if 0 <= rr_idx < n_rows and 0 <= rc_idx < n_cols: \n            tensor[3, rr_idx, rc_idx] = 1.0\n\n    #Channel 4: Current robot's position\n    crr, crc, _ = current_robot_data \n    crr_idx, crc_idx = int(crr) - 1, int(crc) - 1 \n    if 0 <= crr_idx < n_rows and 0 <= crc_idx < n_cols: \n        tensor[4, crr_idx, crc_idx] = 1.0\n\n    #Channel 5: Current robot's carried package target (if robot is carrying)\n    if carried_pkg_id_by_current_robot != 0:\n        if carried_pkg_id_by_current_robot in persistent_packages:\n            pkg_data_carried = persistent_packages[carried_pkg_id_by_current_robot]\n            tr_carried, tc_carried = pkg_data_carried['target_pos'] \n            if 0 <= tr_carried < n_rows and 0 <= tc_carried < n_cols: \n                tensor[5, tr_carried, tc_carried] = 1.0\n\n    return tensor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.799707Z","iopub.execute_input":"2025-05-15T16:39:56.800217Z","iopub.status.idle":"2025-05-15T16:39:56.818968Z","shell.execute_reply.started":"2025-05-15T16:39:56.800192Z","shell.execute_reply":"2025-05-15T16:39:56.818488Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DQNetwork\n","metadata":{}},{"cell_type":"code","source":"class DQNetwork(nn.Module):\n    def __init__(self, observation_shape, action_dim):\n        super(DQNetwork, self).__init__()\n        self.conv1 = nn.Conv2d(observation_shape[0], 16, kernel_size=3, stride=1, padding='same')\n        self.bn1 = nn.BatchNorm2d(16)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n        self.bn2 = nn.BatchNorm2d(32)\n        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding='same')\n        self.bn3 = nn.BatchNorm2d(32)\n        flat_size = 32 * observation_shape[1] * observation_shape[2]  \n\n        self.fc1 = nn.Linear(flat_size, 128)\n        self.fc2 = nn.Linear(128, action_dim)\n\n    def forward(self, obs):\n        # obs: (N, C, H, W) or (C, H, W)\n        if obs.dim() == 3:\n            obs = obs.unsqueeze(0)  # (1, C, H, W)\n        x = F.relu(self.bn1(self.conv1(obs)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = x.flatten(start_dim=1)  # (N, 32*H*W)\n        x = F.relu(self.fc1(x))\n        q_values = self.fc2(x)\n        return q_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.819548Z","iopub.execute_input":"2025-05-15T16:39:56.819807Z","iopub.status.idle":"2025-05-15T16:39:56.847622Z","shell.execute_reply.started":"2025-05-15T16:39:56.819792Z","shell.execute_reply":"2025-05-15T16:39:56.847137Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Replay Buffer","metadata":{}},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, capacity, obs_shape, device=\"cpu\"):\n        self.capacity = capacity\n        self.obs_shape = obs_shape\n        self.device = device\n\n        _obs_s = (obs_shape,) if isinstance(obs_shape, int) else obs_shape\n\n        self.observations = np.zeros((capacity, *_obs_s), dtype=np.float32)\n        self.actions = np.zeros((capacity,), dtype=np.int64)\n        self.rewards = np.zeros((capacity,), dtype=np.float32)\n        self.next_observations = np.zeros((capacity, *_obs_s), dtype=np.float32)\n        self.dones = np.zeros((capacity,), dtype=np.bool_)\n\n        self.ptr = 0\n        self.size = 0\n\n    def add(self, obs, action, reward, next_obs, done):\n        \"\"\"\n        Adds a transition to the buffer.\n        - obs: np.array with shape self.obs_shape\n        - action: int\n        - reward: float\n        - next_obs: np.array with shape self.obs_shape\n        - done: bool\n        \"\"\"\n        self.observations[self.ptr] = obs\n        self.actions[self.ptr] = action\n        self.rewards[self.ptr] = reward\n        self.next_observations[self.ptr] = next_obs\n        self.dones[self.ptr] = done\n\n        self.ptr = (self.ptr + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n\n    def sample(self, batch_size):\n        if self.size == 0:\n            print(\"Buffer is empty. Returned empty tensors\")\n            _obs_s_runtime = (self.obs_shape,) if isinstance(self.obs_shape, int) else self.obs_shape\n            empty_obs = torch.empty((0, *_obs_s_runtime), dtype=torch.float32, device=self.device)\n            empty_actions = torch.empty((0,), dtype=torch.long, device=self.device)\n            empty_rewards = torch.empty((0,), dtype=torch.float32, device=self.device)\n            empty_next_obs = torch.empty((0, *_obs_s_runtime), dtype=torch.float32, device=self.device)\n            empty_dones = torch.empty((0,), dtype=torch.float32, device=self.device)\n            return (empty_obs, empty_actions, empty_rewards, empty_next_obs, empty_dones)\n\n        if self.size < batch_size:\n            print(f\"Warning: Buffer size ({self.size}) is less than batch size ({batch_size}). Sampling all available data.\")\n            indices = np.arange(self.size)\n        else:\n            indices = np.random.choice(self.size, batch_size, replace=False)\n\n        batch_obs = torch.tensor(self.observations[indices], dtype=torch.float32).to(self.device)\n        batch_actions = torch.tensor(self.actions[indices], dtype=torch.long).to(self.device)\n        batch_rewards = torch.tensor(self.rewards[indices], dtype=torch.float32).to(self.device)\n        batch_next_obs = torch.tensor(self.next_observations[indices], dtype=torch.float32).to(self.device)\n        batch_dones = torch.tensor(self.dones[indices], dtype=torch.float32).to(self.device)\n\n        return (batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones)\n\n    def can_sample(self, batch_size):\n        return self.size >= batch_size\n\n    def __len__(self):\n        return self.size","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.848246Z","iopub.execute_input":"2025-05-15T16:39:56.848762Z","iopub.status.idle":"2025-05-15T16:39:56.872152Z","shell.execute_reply.started":"2025-05-15T16:39:56.848739Z","shell.execute_reply":"2025-05-15T16:39:56.871647Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save model","metadata":{}},{"cell_type":"code","source":"def save_model(policy_net, path=\"/kaggle/working/models/dqn_agent.pt\"):\n    if not os.path.exists(os.path.dirname(path)):\n        os.makedirs(os.path.dirname(path))\n    torch.save(policy_net.state_dict(), path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:43:46.711372Z","iopub.execute_input":"2025-05-15T16:43:46.711649Z","iopub.status.idle":"2025-05-15T16:43:46.715934Z","shell.execute_reply.started":"2025-05-15T16:43:46.711630Z","shell.execute_reply":"2025-05-15T16:43:46.715267Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Reward Shaping","metadata":{}},{"cell_type":"code","source":"SHAPING_SUCCESSFUL_PICKUP_BONUS = 1\nSHAPING_SUCCESSFUL_DELIVERY_BONUS = 0 \nSHAPING_WASTED_PICKUP_PENALTY = -1 # Tried to pick from an empty spot or while already carrying\nSHAPING_WASTED_DROP_PENALTY = -1   # Tried to drop when not carrying\n\ndef reward_shaping(global_r, prev_env_state, current_env_state, actions_taken, persistent_packages_before_action, num_agents):\n    \"\"\"\n    Shapes the global reward 'global_r' to produce individual rewards,\n    using a snapshot of the persistent package tracker from before the action.\n\n    Args:\n        global_r (float): The global reward from the environment for the current step (s -> s').\n        prev_env_state (dict): The raw environment state 's' before actions_taken.\n                               Contains 'robots' (1-indexed pos) and 'time_step'.\n        current_env_state (dict): The raw environment state 's'' after actions_taken.\n                                  Contains 'robots' (1-indexed pos) and 'time_step'.\n        actions_taken (list): List of actions [(move_idx, package_op_idx), ...]\n        persistent_packages_before_action (dict): \n            A copy of `self.persistent_packages` before the current action.\n            Structure: { pkg_id: {'id': ..., 'start_pos': (r,c), 'target_pos': (r,c), \n                                   'start_time': ..., 'deadline': ..., \n                                   'status': 'waiting'/'in_transit'}, ... }\n            Positions are 0-indexed. \n        num_agents (int): The number of agents.\n\n    Returns:\n        list: A list of shaped rewards, one for each agent.\n    \"\"\"\n    \n    individual_rewards = [0.0] * num_agents\n\n    for i in range(num_agents):\n        individual_rewards[i] = global_r # Initialize with original reward\n\n    current_time_from_env = current_env_state['time_step']\n\n    if isinstance(current_time_from_env, np.ndarray):\n        current_time_from_env = current_time_from_env[0]\n    \n    # prev_env_state might not have 'time_step'\n    time_at_prev_state = prev_env_state.get('time_step', current_time_from_env -1)\n    if isinstance(time_at_prev_state, np.ndarray):\n        time_at_prev_state = time_at_prev_state[0]\n\n\n    for i in range(num_agents):\n        agent_action = actions_taken[i]\n        package_op = agent_action[1]  # 0: None, 1: Pick, 2: Drop\n\n        if i >= len(prev_env_state['robots']) or i >= len(current_env_state['robots']):\n            continue\n\n        prev_robot_info = prev_env_state['robots'][i]\n        current_robot_info = current_env_state['robots'][i]\n\n        robot_prev_pos_0idx = (prev_robot_info[0] - 1, prev_robot_info[1] - 1)\n        robot_current_pos_0idx = (current_robot_info[0] - 1, current_robot_info[1] - 1)\n\n        prev_carrying_id = prev_robot_info[2]\n        current_carrying_id = current_robot_info[2]\n\n        # 1. Shaping for PICKUP attempts\n        if package_op == 1:\n            if prev_carrying_id == 0 and current_carrying_id != 0:\n                # Successfully picked up a package\n                individual_rewards[i] += SHAPING_SUCCESSFUL_PICKUP_BONUS\n            elif prev_carrying_id != 0:\n                # Tried to pick up while already carrying\n                individual_rewards[i] += SHAPING_WASTED_PICKUP_PENALTY\n            elif prev_carrying_id == 0 and current_carrying_id == 0:\n                # Attempted pickup but failed (still not carrying).\n                # Check if a 'waiting' package was truly available at the robot's previous location.\n                package_was_available_and_waiting = False\n                for pkg_id, pkg_data in persistent_packages_before_action.items():\n                    if pkg_data['status'] == 'waiting' and \\\n                       pkg_data['start_pos'] == robot_prev_pos_0idx and \\\n                       pkg_data['start_time'] <= time_at_prev_state: # Package must be active\n                        package_was_available_and_waiting = True\n                        break\n                if not package_was_available_and_waiting:\n                    individual_rewards[i] += SHAPING_WASTED_PICKUP_PENALTY\n\n        # 2. Shaping for DROP attempts\n        elif package_op == 2:\n   \n\n            \n            if prev_carrying_id == 0:\n                individual_rewards[i] += SHAPING_WASTED_DROP_PENALTY\n                \n    return individual_rewards\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.895856Z","iopub.execute_input":"2025-05-15T16:39:56.896045Z","iopub.status.idle":"2025-05-15T16:39:56.912349Z","shell.execute_reply.started":"2025-05-15T16:39:56.896031Z","shell.execute_reply":"2025-05-15T16:39:56.911845Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"env = Environment(map_file=MAP_FILE,\n                  n_robots=NUM_AGENTS, \n                  n_packages=N_PACKAGES,\n                  move_cost=MOVE_COST,\n                  delivery_reward=DELIVERY_REWARD,\n                  delay_reward=DELAY_REWARD,\n                  seed=SEED,\n                  max_time_steps=MAX_TIME_STEPS)\nenv.reset()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.912939Z","iopub.execute_input":"2025-05-15T16:39:56.913212Z","iopub.status.idle":"2025-05-15T16:39:56.944976Z","shell.execute_reply.started":"2025-05-15T16:39:56.913186Z","shell.execute_reply":"2025-05-15T16:39:56.944349Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Trainer","metadata":{}},{"cell_type":"code","source":"class DQNTrainer:\n    def __init__(self, env, lr=LR, weight_decay=WEIGHT_DECAY, gamma=GAMMA, tau=TAU, gradient_clipping=GRADIENT_CLIPPING):\n        self.env = env\n        OBS_DIM = (6, env.n_rows, env.n_cols ) \n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        \n        # Initialize LabelEncoders for actions\n        self.le_move = LabelEncoder()\n        self.le_move.fit(['S', 'L', 'R', 'U', 'D']) # Stay, Left, Right, Up, Down\n        self.le_pkg_op = LabelEncoder()\n        self.le_pkg_op.fit(['0', '1', '2']) # 0: None, 1: Pickup, 2: Drop\n        self.NUM_MOVE_ACTIONS = len(self.le_move.classes_) # 5\n        self.NUM_PKG_OPS = len(self.le_pkg_op.classes_) # 3\n        \n        # Network\n        self.agent_network = DQNetwork(OBS_DIM, ACTION_DIM).to(self.device)\n        \n        # Target networks\n        self.target_agent_network = DQNetwork(OBS_DIM, ACTION_DIM).to(self.device)\n        self.target_agent_network.load_state_dict(self.agent_network.state_dict())\n        \n        self.buffer = ReplayBuffer(capacity=MAX_REPLAY_BUFFER_SIZE, obs_shape=OBS_DIM, device=self.device)\n        self.lr = lr\n        self.weight_decay = weight_decay\n        self.gamma = gamma\n        self.tau = tau\n        self.gradient_clipping = gradient_clipping\n        \n        # Persistent packages to track the package id and the target position\n        self.persistent_packages = {}\n\n        self.update_targets(1.0)  # Hard update at start\n\n        self.agent_optimizer = optim.Adam(self.agent_network.parameters(), lr=lr, weight_decay=weight_decay)\n        \n    def _update_persistent_packages(self, current_env_state): \n        \"\"\"\n        Updates self.persistent_packages based on the current environment state.\n        - current_env_state: The state dictionary from env.step() or env.reset().\n        \"\"\"\n        # 1. Add newly appeared packages to persistent_packages if not already tracked\n        if 'packages' in current_env_state and current_env_state['packages'] is not None:\n            for pkg_tuple in current_env_state['packages']:\n                pkg_id = pkg_tuple[0]\n                if pkg_id not in self.persistent_packages:\n                    self.persistent_packages[pkg_id] = {\n                        'id': pkg_id,\n                        'start_pos': (pkg_tuple[1] - 1, pkg_tuple[2] - 1),\n                        'target_pos': (pkg_tuple[3] - 1, pkg_tuple[4] - 1),\n                        'start_time': pkg_tuple[5],\n                        'deadline': pkg_tuple[6],\n                        'status': 'waiting'\n                    }\n\n        # 2. Get current robot carrying info\n        current_carried_pkg_ids_set = set()\n        if 'robots' in current_env_state and current_env_state['robots'] is not None:\n            for r_idx, r_data in enumerate(current_env_state['robots']):\n                carried_id = r_data[2] # (pos_x+1, pos_y+1, carrying_package_id)\n                if carried_id != 0:\n                    current_carried_pkg_ids_set.add(carried_id)\n\n        packages_to_remove_definitively = []\n\n        # 3. Update package status\n        for pkg_id, pkg_data in list(self.persistent_packages.items()):\n            original_status_in_tracker = pkg_data['status']\n\n            if pkg_id in current_carried_pkg_ids_set:\n                # If currently being carried by any robot in current_env_state, set to 'in_transit'\n                self.persistent_packages[pkg_id]['status'] = 'in_transit'\n            else:\n                # Package is NOT being carried in current_env_state\n                if original_status_in_tracker == 'in_transit':\n                    # This package WAS 'in_transit' (according to our tracker)\n                    # and is now NOT carried in current_env_state.\n                    # Given the env.py logic, this means it MUST have been delivered correctly.\n                    packages_to_remove_definitively.append(pkg_id)\n                # If original_status_in_tracker was 'waiting' and it's still not carried,\n                # its status remains 'waiting'. No change needed to start_pos or status here.\n                pass\n\n        # 4. Remove packages that were successfully delivered\n        for pkg_id_to_remove in packages_to_remove_definitively:\n            if pkg_id_to_remove in self.persistent_packages:\n                del self.persistent_packages[pkg_id_to_remove]\n\n\n    def update_targets(self, tau=None):\n        if tau is None:\n            tau = self.tau\n        # Soft update\n        for target_param, param in zip(self.target_agent_network.parameters(), self.agent_network.parameters()):\n            target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)\n\n\n    def select_action(self, obs, eps):\n        # obs_batch: (C, H, W)\n        if obs is not isinstance(obs, torch.Tensor) and isinstance(obs, np.ndarray):\n            obs = torch.from_numpy(obs).float().to(self.device)\n        if np.random.rand() < eps:\n            action = np.random.randint(0, ACTION_DIM)\n        else:\n            with torch.no_grad():\n                q_values = self.agent_network(obs)\n                action = torch.argmax(q_values, dim=1).item()\n        return action\n\n    def train_step(self, batch_size):\n        if not self.buffer.can_sample(batch_size):\n            return None\n\n        batch_obs, batch_actions, batch_rewards, batch_next_obs, batch_dones = self.buffer.sample(batch_size)\n        # batch_obs: (B, C, H, W)\n        # batch_actions: (B,)\n        # batch_rewards: (B,)\n        # batch_next_obs: (B, C, H, W)\n        # batch_dones: (B,)\n\n        # Compute Q(s, a) for the actions taken\n        q_values = self.agent_network(batch_obs)  # (B, action_dim)\n        q_value = q_values.gather(1, batch_actions.unsqueeze(1)).squeeze(1)  # (B,)\n\n        # Compute target Q values\n        with torch.no_grad():\n            next_q_values = self.target_agent_network(batch_next_obs)  # (B, action_dim)\n            next_q_value = next_q_values.max(1)[0]  # (B,)\n            target = batch_rewards + self.gamma * (1 - batch_dones) * next_q_value\n\n        # Loss\n        loss = F.mse_loss(q_value, target)\n\n        # Optimize\n        self.agent_optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.agent_network.parameters(), self.gradient_clipping)\n        self.agent_optimizer.step()\n\n        # Soft update target network\n        self.update_targets(self.tau)\n\n        return loss.item()\n\n\n    def run_episode(self, eps):\n        current_state_dict = self.env.reset()\n        self.persistent_packages = {}\n        self._update_persistent_packages(current_state_dict)\n        done = False\n        episode_reward = 0\n        episode_loss = 0\n        step_count = 0\n\n        while not done:\n            # self.env.render_pygame()\n            # Build per-agent observations\n            actions = []\n            observations = []\n            env_actions = []\n            packages_before_action = copy.deepcopy(self.persistent_packages)\n            \n            for i in range(NUM_AGENTS):\n                obs = convert_state(current_state_dict, self.persistent_packages, current_robot_idx=i)\n                observations.append(obs)\n                action = self.select_action(obs, eps)\n                actions.append(action)\n            next_observations = []\n            prev_state_dict = current_state_dict\n            \n            # Take actions and get next state\n            for int_act in actions:\n                move_idx = int_act % self.NUM_MOVE_ACTIONS\n                pkg_op_idx = int_act // self.NUM_MOVE_ACTIONS\n\n                # Ensure pkg_op_idx is within bounds\n                if pkg_op_idx >= self.NUM_PKG_OPS:\n                    print(f\"Warning: Decoded pkg_op_idx {pkg_op_idx} is out of bounds for action {int_act}. Max is {self.NUM_PKG_OPS-1}. Defaulting to op index 0.\")\n                    pkg_op_idx = 0 # Default to the first package operation (e.g., 'None')\n                \n                move_str = self.le_move.inverse_transform([move_idx])[0]\n                pkg_op_str = self.le_pkg_op.inverse_transform([pkg_op_idx])[0]\n                env_actions.append((move_str, pkg_op_str))\n                \n                \n            current_state_dict, global_reward, done, _= self.env.step(env_actions)\n            individual_rewards = reward_shaping(global_reward, \n                                                prev_state_dict, \n                                                current_state_dict, \n                                                env_actions,\n                                                packages_before_action,\n                                                NUM_AGENTS)\n            self._update_persistent_packages(current_state_dict)\n            \n            # Build per-agent next observations\n            for i in range(NUM_AGENTS):\n                next_obs = convert_state(current_state_dict, self.persistent_packages, current_robot_idx=i)\n                next_observations.append(next_obs)\n\n            # Store in buffer\n            for i in range(NUM_AGENTS):\n                self.buffer.add(\n                    obs=observations[i],\n                    action=actions[i],\n                    reward=individual_rewards[i],\n                    next_obs=next_observations[i],\n                    done=done\n                )\n\n            episode_reward += global_reward\n            step_count += 1\n\n            # Training step\n            loss = self.train_step(BATCH_SIZE)\n            if loss is not None:\n                episode_loss += loss\n\n\n        return episode_reward, episode_loss / max(1, step_count)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:39:56.945724Z","iopub.execute_input":"2025-05-15T16:39:56.946427Z","iopub.status.idle":"2025-05-15T16:39:56.966868Z","shell.execute_reply.started":"2025-05-15T16:39:56.946405Z","shell.execute_reply":"2025-05-15T16:39:56.966395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"trainer = DQNTrainer(env)\ndef linear_epsilon(steps_done):\n    return max(EPS_END, EPS_START - (EPS_START - EPS_END) * (steps_done / EPS_DECAY))\n\n\ndef exponential_epsilon(steps_done):\n    return EPS_END + (EPS_START - EPS_END) * np.exp(-steps_done / EPS_DECAY)\n\nsteps_done = np.arange(NUM_EPISODES)\n\nlinear_epsilons = [linear_epsilon(step) for step in steps_done]\nexp_epsilons = [exponential_epsilon(step) for step in steps_done]\n\n# Lists to store metrics for plotting\nepisode_rewards_history = []\nepisode_avg_loss_history = []\n\ntraining_completed_successfully = False\nprint(\"Starting DQN training...\")\nprint(f\"Running for {NUM_EPISODES} episodes.\")\n\ntry:\n    best_reward = -999\n    for episode_num in range(1, NUM_EPISODES + 1):\n        # The exponential_epsilon function from in[16] expects 'steps_done'\n        # Assuming 'steps_done' in that context refers to the number of episodes completed (0-indexed)\n        current_epsilon = linear_epsilon(episode_num - 1) \n        \n        episode_reward, avg_episode_loss = trainer.run_episode(current_epsilon)\n        \n        episode_rewards_history.append(episode_reward)\n        episode_avg_loss_history.append(avg_episode_loss)\n        if episode_reward > best_reward:\n            best_reward = episode_reward\n            print(f\"Save best model at episode {episode_num} with best_reward {best_reward}\")\n            save_model(trainer.agent_network, path=f\"/kaggle/working/models/dqn_agent_best_reward.pt\")\n                \n        if episode_num % 10 == 0 or episode_num == NUM_EPISODES: # Print every 10 episodes and the last one\n            print(f\"Episode {episode_num}/{NUM_EPISODES} | Reward: {episode_reward:.2f} | Avg Loss: {avg_episode_loss:.4f} | Epsilon: {current_epsilon:.3f}\")\n\n        # Optional: Periodic saving during training\n        if episode_num % 50 == 0: # Example: Save every 50 episodes\n            print(f\"Saving checkpoint at episode {episode_num}...\")\n            save_model(trainer.agent_network, path=f\"/kaggle/working/models/dqn_agent_ep{episode_num}.pt\")\n            \n    training_completed_successfully = True\n\nexcept KeyboardInterrupt:\n    print(\"\\nTraining interrupted by user (KeyboardInterrupt).\")\n    print(\"Saving current model state...\")\n    save_model(trainer.agent_network, path=\"/kaggle/working/models/dqn_agent_interrupted.pt\")\n    print(\"Models saved to _interrupted.pt files.\")\nexcept Exception as e:\n    print(f\"\\nAn error occurred during training: {e}\")\n    import traceback\n    traceback.print_exc() # Print detailed traceback for the exception\n    print(\"Saving model due to exception...\")\n    save_model(trainer.agent_network, path=\"/kaggle/working/models/dqn_agent_exception.pt\")\n    print(\"Models saved to _exception.pt files.\")\nfinally:\n    # pygame.quit()\n    print(\"\\nTraining loop finished or was interrupted.\")\n    \n    # Plotting the results\n    if episode_rewards_history: # Check if there's any data to plot\n        plt.figure(figsize=(14, 6))\n\n        plt.subplot(1, 2, 1)\n        plt.plot(episode_rewards_history)\n        plt.title('Total Reward per Episode')\n        plt.xlabel('Episode')\n        plt.ylabel('Total Reward')\n        plt.grid(True)\n\n        plt.subplot(1, 2, 2)\n        plt.plot(episode_avg_loss_history)\n        plt.title('Average Loss per Episode')\n        plt.xlabel('Episode')\n        plt.ylabel('Average Loss')  \n        plt.grid(True)\n\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(\"No data recorded for plotting.\")\n\nif training_completed_successfully:\n    print(\"\\nTraining completed successfully.\")\n    print(\"Saving final model...\")\n    save_model(trainer.agent_network, path=\"/kaggle/working/models/dqn_agent_final.pt\")\n    print(\"Final models saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T18:07:45.874751Z","iopub.execute_input":"2025-05-15T18:07:45.874993Z","execution_failed":"2025-05-15T18:12:49.710Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}